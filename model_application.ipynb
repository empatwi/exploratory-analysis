{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>keyword</th>\n",
       "      <th>user_location</th>\n",
       "      <th>classified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mds nunca liberar</td>\n",
       "      <td>['Nubank']</td>\n",
       "      <td>Goiânia</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>viciar bateria celular</td>\n",
       "      <td>['Coca-cola']</td>\n",
       "      <td>Balneário Camboriú</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toda vez vejo lá conferir saldo certo juro tra...</td>\n",
       "      <td>['Nubank']</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pedir serasi</td>\n",
       "      <td>['Nubank']</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kayofeer acontecer mãe quase ano nada ainda</td>\n",
       "      <td>['Nubank']</td>\n",
       "      <td>Macaé, Brasil</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweet_content        keyword  \\\n",
       "0                                  mds nunca liberar     ['Nubank']   \n",
       "1                             viciar bateria celular  ['Coca-cola']   \n",
       "2  toda vez vejo lá conferir saldo certo juro tra...     ['Nubank']   \n",
       "3                                       pedir serasi     ['Nubank']   \n",
       "4        kayofeer acontecer mãe quase ano nada ainda     ['Nubank']   \n",
       "\n",
       "            user_location  classified  \n",
       "0                 Goiânia         0.0  \n",
       "1      Balneário Camboriú         1.0  \n",
       "2                    None         0.0  \n",
       "3  Rio de Janeiro, Brasil         1.0  \n",
       "4           Macaé, Brasil         1.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "df_dataset = pd.read_csv('files/clean_text.csv')\n",
    "df_dataset = df_dataset.drop(['Unnamed: 0'], axis=1)\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "X = count_vectorizer.fit_transform(df_dataset['tweet_content'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaaaa',\n",
       " 'abandonar',\n",
       " 'abençoar',\n",
       " 'abismal',\n",
       " 'abrir',\n",
       " 'abro',\n",
       " 'absurda',\n",
       " 'absurdo',\n",
       " 'abóbora',\n",
       " 'acabar',\n",
       " 'academia',\n",
       " 'acaso',\n",
       " 'aceitar',\n",
       " 'acertar',\n",
       " 'acerto',\n",
       " 'acesso',\n",
       " 'achar',\n",
       " 'acho',\n",
       " 'ackisamayra',\n",
       " 'acontecer',\n",
       " 'acordar',\n",
       " 'acostumar',\n",
       " 'acreditar',\n",
       " 'acredito',\n",
       " 'adiantar',\n",
       " 'adidasbrasil',\n",
       " 'adiirjr',\n",
       " 'adir',\n",
       " 'adoro',\n",
       " 'afrontar',\n",
       " 'agasalho',\n",
       " 'agilizar',\n",
       " 'agiotar',\n",
       " 'agir',\n",
       " 'agoniar',\n",
       " 'agora',\n",
       " 'agr',\n",
       " 'agraciar',\n",
       " 'agradecida',\n",
       " 'agradeço',\n",
       " 'aguardar',\n",
       " 'aguardo',\n",
       " 'aguentar',\n",
       " 'agência',\n",
       " 'ah',\n",
       " 'ahah',\n",
       " 'ahhh',\n",
       " 'ahora',\n",
       " 'aiii',\n",
       " 'ainda',\n",
       " 'aipords',\n",
       " 'air',\n",
       " 'ajuda',\n",
       " 'ajudar',\n",
       " 'ajude',\n",
       " 'akbilbo',\n",
       " 'akendall',\n",
       " 'alar',\n",
       " 'alexandreelf',\n",
       " 'algo',\n",
       " 'alguem',\n",
       " 'algum',\n",
       " 'alguma',\n",
       " 'algumas',\n",
       " 'alguns',\n",
       " 'alguém',\n",
       " 'ali',\n",
       " 'aliexpress',\n",
       " 'aliexpressbr',\n",
       " 'alimentação',\n",
       " 'all',\n",
       " 'allstar',\n",
       " 'alta',\n",
       " 'alterar',\n",
       " 'alto',\n",
       " 'alwaysmeteoro',\n",
       " 'além',\n",
       " 'alô',\n",
       " 'amar',\n",
       " 'amaralgusman',\n",
       " 'amazon',\n",
       " 'amazonilio',\n",
       " 'ambev',\n",
       " 'ambiguo',\n",
       " 'ameaçar',\n",
       " 'america',\n",
       " 'americano',\n",
       " 'amiga',\n",
       " 'amigos',\n",
       " 'amo',\n",
       " 'amor',\n",
       " 'amp',\n",
       " 'amém',\n",
       " 'américa',\n",
       " 'anal',\n",
       " 'anda',\n",
       " 'andar',\n",
       " 'andrezzin_cec',\n",
       " 'android',\n",
       " 'aniversario',\n",
       " 'aniversário',\n",
       " 'anjooo',\n",
       " 'ano',\n",
       " 'anos',\n",
       " 'ansiosa',\n",
       " 'ansioso',\n",
       " 'antartica',\n",
       " 'anterior',\n",
       " 'antes',\n",
       " 'antix',\n",
       " 'antártica',\n",
       " 'anuncio',\n",
       " 'aoi_kawai',\n",
       " 'aonde',\n",
       " 'ap',\n",
       " 'apagar',\n",
       " 'apaixonada',\n",
       " 'aparaceu',\n",
       " 'aparecer',\n",
       " 'aplicativo',\n",
       " 'apostar',\n",
       " 'app',\n",
       " 'apple',\n",
       " 'applepay',\n",
       " 'aprender',\n",
       " 'apresentar',\n",
       " 'aprovar',\n",
       " 'aproveitamento',\n",
       " 'aproveitar',\n",
       " 'aq',\n",
       " 'aquellos',\n",
       " 'ar',\n",
       " 'arcuri',\n",
       " 'ardido',\n",
       " 'areas',\n",
       " 'argiloso',\n",
       " 'arohadorameira',\n",
       " 'arohamaknae',\n",
       " 'arrepender',\n",
       " 'arrumar',\n",
       " 'assim',\n",
       " 'assinar',\n",
       " 'assistente',\n",
       " 'assunto',\n",
       " 'ataquemarketing',\n",
       " 'ate',\n",
       " 'atemporal',\n",
       " 'atendimento',\n",
       " 'ater',\n",
       " 'atlético',\n",
       " 'atoa',\n",
       " 'atual',\n",
       " 'atualizada',\n",
       " 'atualização',\n",
       " 'atualizei',\n",
       " 'aulas',\n",
       " 'aumenta',\n",
       " 'aumentaaaa',\n",
       " 'aumentar',\n",
       " 'aumente',\n",
       " 'aumento',\n",
       " 'aumentou',\n",
       " 'aylasol',\n",
       " 'azar',\n",
       " 'babar',\n",
       " 'bagulho',\n",
       " 'bahia',\n",
       " 'baixar',\n",
       " 'baixei',\n",
       " 'baixo',\n",
       " 'balançar',\n",
       " 'bananeira',\n",
       " 'banco',\n",
       " 'bancos',\n",
       " 'banrisul',\n",
       " 'baratinhas',\n",
       " 'barcelona',\n",
       " 'barras',\n",
       " 'barriga',\n",
       " 'barrotado',\n",
       " 'base',\n",
       " 'bastante',\n",
       " 'bata',\n",
       " 'batata',\n",
       " 'bater',\n",
       " 'bateria',\n",
       " 'bcredi',\n",
       " 'beautaefu',\n",
       " 'beber',\n",
       " 'bebo',\n",
       " 'bem',\n",
       " 'berenice',\n",
       " 'bermuda',\n",
       " 'besta',\n",
       " 'besteira',\n",
       " 'beyoung',\n",
       " 'bico',\n",
       " 'bih',\n",
       " 'biju',\n",
       " 'bis',\n",
       " 'bixo',\n",
       " 'blazer',\n",
       " 'blazerkkj',\n",
       " 'blazers',\n",
       " 'blogdosaopaulo',\n",
       " 'bloquear',\n",
       " 'blusa',\n",
       " 'blusinha',\n",
       " 'blusão',\n",
       " 'bnakamatsu',\n",
       " 'bo',\n",
       " 'boa',\n",
       " 'bobo',\n",
       " 'bobojaco',\n",
       " 'boca',\n",
       " 'bocar',\n",
       " 'bola',\n",
       " 'bolar',\n",
       " 'bolas',\n",
       " 'boleto',\n",
       " 'boletos',\n",
       " 'bolo',\n",
       " 'bolso',\n",
       " 'bom',\n",
       " 'bonito',\n",
       " 'bons',\n",
       " 'bonzinho',\n",
       " 'bonéfogo',\n",
       " 'boonbap',\n",
       " 'boot',\n",
       " 'bootzin',\n",
       " 'boquinha',\n",
       " 'bora',\n",
       " 'bosta',\n",
       " 'bot',\n",
       " 'bota',\n",
       " 'boy',\n",
       " 'braba',\n",
       " 'brabo',\n",
       " 'branco',\n",
       " 'brasil',\n",
       " 'brasileiro',\n",
       " 'bravo',\n",
       " 'brechozao',\n",
       " 'brendabremm',\n",
       " 'brendinhakonig',\n",
       " 'brincadeira',\n",
       " 'brincar',\n",
       " 'brinco',\n",
       " 'brotar',\n",
       " 'brt',\n",
       " 'brtt',\n",
       " 'brás',\n",
       " 'buceta',\n",
       " 'buenas',\n",
       " 'bug',\n",
       " 'bunda',\n",
       " 'buracão',\n",
       " 'burguesia',\n",
       " 'burguêsa',\n",
       " 'buscar',\n",
       " 'but',\n",
       " 'cabelo',\n",
       " 'cabelos',\n",
       " 'cabeça',\n",
       " 'cachorro',\n",
       " 'cada',\n",
       " 'cadê',\n",
       " 'cafe',\n",
       " 'café',\n",
       " 'cagar',\n",
       " 'cair',\n",
       " 'caixa',\n",
       " 'caixadepandoriapgr',\n",
       " 'caiçara',\n",
       " 'calcar',\n",
       " 'caldo',\n",
       " 'calvin',\n",
       " 'calça',\n",
       " 'calçar',\n",
       " 'cama',\n",
       " 'caminho',\n",
       " 'caminhão',\n",
       " 'camisa',\n",
       " 'camisas',\n",
       " 'camisola',\n",
       " 'campanha',\n",
       " 'campeão',\n",
       " 'cancelar',\n",
       " 'cansada',\n",
       " 'capinhas',\n",
       " 'capotar',\n",
       " 'cara',\n",
       " 'caralho',\n",
       " 'carnaval',\n",
       " 'caro',\n",
       " 'carregar',\n",
       " 'carrinho',\n",
       " 'cartao',\n",
       " 'carteira',\n",
       " 'carter',\n",
       " 'cartomaniacosfc',\n",
       " 'cartão',\n",
       " 'cartões',\n",
       " 'caráter',\n",
       " 'casa',\n",
       " 'casaco',\n",
       " 'caseiro',\n",
       " 'caso',\n",
       " 'catastrófica',\n",
       " 'causa',\n",
       " 'cavalo',\n",
       " 'caí',\n",
       " 'cdg',\n",
       " 'cello_silva',\n",
       " 'celta',\n",
       " 'celula',\n",
       " 'celular',\n",
       " 'celulite',\n",
       " 'cenas',\n",
       " 'cenoura',\n",
       " 'cent',\n",
       " 'centauro',\n",
       " 'centauroesporte',\n",
       " 'centavo',\n",
       " 'centavos',\n",
       " 'center',\n",
       " 'cereja',\n",
       " 'cerolzera',\n",
       " 'certeza',\n",
       " 'certinho',\n",
       " 'certo',\n",
       " 'chamado',\n",
       " 'chamar',\n",
       " 'chat',\n",
       " 'chato',\n",
       " 'chegar',\n",
       " 'chegue',\n",
       " 'cheio',\n",
       " 'chkhwa',\n",
       " 'chocar',\n",
       " 'chocolate',\n",
       " 'choque',\n",
       " 'chorar',\n",
       " 'churrasco',\n",
       " 'chuteira',\n",
       " 'chuteiras',\n",
       " 'cidade',\n",
       " 'cima',\n",
       " 'city',\n",
       " 'claro',\n",
       " 'classificação',\n",
       " 'cleytxn',\n",
       " 'club',\n",
       " 'clubes',\n",
       " 'cm',\n",
       " 'cobertura',\n",
       " 'cobrar',\n",
       " 'cocacola_br',\n",
       " 'coco',\n",
       " 'coisa',\n",
       " 'coisado',\n",
       " 'coisas',\n",
       " 'coisinhas',\n",
       " 'coitado',\n",
       " 'coleira',\n",
       " 'coleção',\n",
       " 'colina',\n",
       " 'colocar',\n",
       " 'coloco',\n",
       " 'coloquei',\n",
       " 'combina',\n",
       " 'combinar',\n",
       " 'combo',\n",
       " 'comentar',\n",
       " 'comer',\n",
       " 'começar',\n",
       " 'começo',\n",
       " 'comodismo',\n",
       " 'competição',\n",
       " 'completamente',\n",
       " 'completo',\n",
       " 'complicar',\n",
       " 'compra',\n",
       " 'comprar',\n",
       " 'compras',\n",
       " 'comprinhas',\n",
       " 'compro',\n",
       " 'computador',\n",
       " 'comunidade',\n",
       " 'conceito',\n",
       " 'concentrar',\n",
       " 'concluir',\n",
       " 'concordar',\n",
       " 'concorrente',\n",
       " 'conferir',\n",
       " 'confia',\n",
       " 'confirmar',\n",
       " 'confiável',\n",
       " 'confusa',\n",
       " 'confusas',\n",
       " 'conhecer',\n",
       " 'conjunto',\n",
       " 'conquistar',\n",
       " 'conseguir',\n",
       " 'consigo',\n",
       " 'consumista',\n",
       " 'conta',\n",
       " 'contar',\n",
       " 'contato',\n",
       " 'contestar',\n",
       " 'continuar',\n",
       " 'conto',\n",
       " 'contra',\n",
       " 'contrata',\n",
       " 'contratar',\n",
       " 'contrato',\n",
       " 'convidar',\n",
       " 'copinho',\n",
       " 'copo',\n",
       " 'coração',\n",
       " 'cordão',\n",
       " 'corinthians',\n",
       " 'coritiba',\n",
       " 'corpo',\n",
       " 'corrente',\n",
       " 'correr',\n",
       " 'corretora',\n",
       " 'court',\n",
       " 'covid',\n",
       " 'coxinha',\n",
       " 'coxinhas',\n",
       " 'coçar',\n",
       " 'cpx',\n",
       " 'credita',\n",
       " 'credo',\n",
       " 'crescer',\n",
       " 'criança',\n",
       " 'crianças',\n",
       " 'criar',\n",
       " 'criatura',\n",
       " 'cristóvão',\n",
       " 'crisvector',\n",
       " 'crl',\n",
       " 'cropped',\n",
       " 'cruzeiro',\n",
       " 'crédito',\n",
       " 'ctz',\n",
       " 'cu',\n",
       " 'cueca',\n",
       " 'cuidar',\n",
       " 'curitiba',\n",
       " 'cyclone',\n",
       " 'cássio',\n",
       " 'câmera',\n",
       " 'cão',\n",
       " 'cérebro',\n",
       " 'céu',\n",
       " 'cês',\n",
       " 'código',\n",
       " 'daddy',\n",
       " 'daniellmrc',\n",
       " 'daqueles',\n",
       " 'dar',\n",
       " 'datilografa',\n",
       " 'dearschafer',\n",
       " 'decchanamite',\n",
       " 'dedinho',\n",
       " 'deezer',\n",
       " 'degradezin',\n",
       " 'deixar',\n",
       " 'delivery',\n",
       " 'delta',\n",
       " 'demais',\n",
       " 'demonstrar',\n",
       " 'demora',\n",
       " 'demorar',\n",
       " 'dentre',\n",
       " 'depender',\n",
       " 'deposita',\n",
       " 'depositar',\n",
       " 'der',\n",
       " 'des',\n",
       " 'desanimar',\n",
       " 'desaparecer',\n",
       " 'desbloquear',\n",
       " 'descer',\n",
       " 'descobri',\n",
       " 'descontar',\n",
       " 'descubro',\n",
       " 'desculpar',\n",
       " 'desde',\n",
       " 'desejar',\n",
       " 'desejos',\n",
       " 'desempregado',\n",
       " 'desenvolver',\n",
       " 'desgrama',\n",
       " 'design',\n",
       " 'desisti',\n",
       " 'desistir',\n",
       " 'desleal',\n",
       " 'después',\n",
       " 'dessa',\n",
       " 'desse',\n",
       " 'desviei',\n",
       " 'deszanti',\n",
       " 'desânimo',\n",
       " 'deus',\n",
       " 'dever',\n",
       " 'devir',\n",
       " 'dezembro',\n",
       " 'df',\n",
       " 'dia',\n",
       " 'diabitta',\n",
       " 'diabo',\n",
       " 'dias',\n",
       " 'diegosudariop',\n",
       " 'diferenciar',\n",
       " 'diferença',\n",
       " 'difícil',\n",
       " 'digo',\n",
       " 'diminuir',\n",
       " 'dinheiro',\n",
       " 'diogo',\n",
       " 'disfarçadin',\n",
       " 'disgraça',\n",
       " 'dispensar',\n",
       " 'disponível',\n",
       " 'disposto',\n",
       " 'diversas',\n",
       " 'diversos',\n",
       " 'divi',\n",
       " 'dizer',\n",
       " 'dm',\n",
       " 'dms',\n",
       " 'dnv',\n",
       " 'dobrar',\n",
       " 'doer',\n",
       " 'doida',\n",
       " 'doido',\n",
       " 'dois',\n",
       " 'dona',\n",
       " 'dont',\n",
       " 'dor',\n",
       " 'dormir',\n",
       " 'doses',\n",
       " 'dps',\n",
       " 'dragooncrf',\n",
       " 'drx',\n",
       " 'duas',\n",
       " 'dudiskkjk',\n",
       " 'dunkzinho',\n",
       " 'durante',\n",
       " 'duvida',\n",
       " 'duvidar',\n",
       " 'débito',\n",
       " 'dívida',\n",
       " 'dúvida',\n",
       " 'dúvidas',\n",
       " 'eae_guizin',\n",
       " 'eaeew',\n",
       " 'eai',\n",
       " 'eboy',\n",
       " 'ed',\n",
       " 'eeeeguas',\n",
       " 'ei',\n",
       " 'eimattt',\n",
       " 'ein',\n",
       " 'eis',\n",
       " 'elecante',\n",
       " 'eleito',\n",
       " 'eleitores',\n",
       " 'elixir',\n",
       " 'ellen',\n",
       " 'ellos',\n",
       " 'email',\n",
       " 'emprego',\n",
       " 'empresa',\n",
       " 'empresas',\n",
       " 'empresta',\n",
       " 'emprestar',\n",
       " 'empréstimo',\n",
       " 'encher',\n",
       " 'encomenda',\n",
       " 'encomendas',\n",
       " 'encontrar',\n",
       " 'encontro',\n",
       " 'endividar',\n",
       " 'enfim',\n",
       " 'engajamentocec',\n",
       " 'enganar',\n",
       " 'enjoei',\n",
       " 'enrolar',\n",
       " 'ent',\n",
       " 'entao',\n",
       " 'entender',\n",
       " 'entrar',\n",
       " 'entregar',\n",
       " 'entro',\n",
       " 'enviar',\n",
       " 'envolver',\n",
       " 'erickjunio',\n",
       " 'erro',\n",
       " 'es',\n",
       " 'esbanjar',\n",
       " 'escolanosense',\n",
       " 'escolher',\n",
       " 'escravo',\n",
       " 'espaiado',\n",
       " 'espaço',\n",
       " 'especial',\n",
       " 'espera',\n",
       " 'esperar',\n",
       " 'espero',\n",
       " 'espremer',\n",
       " 'espécie',\n",
       " 'espírito',\n",
       " 'esquecer',\n",
       " 'esquecido',\n",
       " 'estagiários',\n",
       " 'estalar',\n",
       " 'estilo',\n",
       " 'estomago',\n",
       " 'estoy',\n",
       " 'estrada',\n",
       " 'estranho',\n",
       " 'estratégia',\n",
       " 'estressadas',\n",
       " 'estuviste',\n",
       " 'estômago',\n",
       " 'estúdio',\n",
       " 'eumatheussr',\n",
       " 'europa',\n",
       " 'euuu',\n",
       " 'evitar',\n",
       " 'exceder',\n",
       " 'excelente',\n",
       " 'exclusiva',\n",
       " 'exclusivas',\n",
       " 'exercer',\n",
       " 'existencial',\n",
       " 'existir',\n",
       " 'existência',\n",
       " 'experimenta',\n",
       " 'experimentar',\n",
       " 'explicar',\n",
       " 'explico',\n",
       " 'explodir',\n",
       " 'exposição',\n",
       " 'extremamente',\n",
       " 'fabricada',\n",
       " 'fabricar',\n",
       " 'facilmente',\n",
       " 'faculdade',\n",
       " 'fadiga',\n",
       " 'fadinhadd',\n",
       " 'faixa',\n",
       " 'falar',\n",
       " 'falaria',\n",
       " 'falir',\n",
       " 'falso',\n",
       " 'falta',\n",
       " 'faltar',\n",
       " 'falto',\n",
       " 'família',\n",
       " 'fantasiar',\n",
       " 'fardinho',\n",
       " 'fase',\n",
       " 'fatura',\n",
       " 'faturas',\n",
       " 'favor',\n",
       " 'favoritada',\n",
       " 'favoritos',\n",
       " 'fazer',\n",
       " 'fb',\n",
       " 'fckggud',\n",
       " 'fdp',\n",
       " 'fds',\n",
       " 'feat',\n",
       " 'fechar',\n",
       " 'feedback',\n",
       " 'feia',\n",
       " 'feio',\n",
       " 'feira',\n",
       " 'felicidad',\n",
       " 'felipevinha',\n",
       " 'feliz',\n",
       " 'felling',\n",
       " 'feminina',\n",
       " 'ferrari',\n",
       " 'ficada',\n",
       " 'ficar',\n",
       " 'fico',\n",
       " 'fight',\n",
       " 'fila',\n",
       " 'filha',\n",
       " 'filhao',\n",
       " 'fim',\n",
       " 'final',\n",
       " 'finalizar',\n",
       " 'finalmente',\n",
       " 'financiar',\n",
       " 'fingir',\n",
       " 'fio',\n",
       " 'fire',\n",
       " 'fit',\n",
       " 'flamengo',\n",
       " 'flooda',\n",
       " 'fluminense',\n",
       " 'focar',\n",
       " 'foda',\n",
       " 'fodase',\n",
       " 'foder',\n",
       " 'folha',\n",
       " 'fone',\n",
       " 'force',\n",
       " 'forever',\n",
       " 'forma',\n",
       " 'formatura',\n",
       " 'forte',\n",
       " 'força',\n",
       " 'fotinhas',\n",
       " 'foto',\n",
       " 'fotografia',\n",
       " 'fraca',\n",
       " 'free',\n",
       " 'frequentar',\n",
       " 'frete',\n",
       " 'fria',\n",
       " 'fridasdaughter',\n",
       " 'frifasnews',\n",
       " 'frio',\n",
       " 'frita',\n",
       " 'frito',\n",
       " 'fudeu',\n",
       " 'fugir',\n",
       " 'fuik',\n",
       " 'fumar',\n",
       " 'fumo',\n",
       " 'funcionalidades',\n",
       " 'funcionar',\n",
       " 'fundo',\n",
       " 'furado',\n",
       " 'furia',\n",
       " 'futebol',\n",
       " 'futebol_info',\n",
       " 'fx',\n",
       " 'fácil',\n",
       " 'fé',\n",
       " 'fêmea',\n",
       " 'galometaldiret',\n",
       " 'ganhar',\n",
       " 'gastar',\n",
       " 'gastos',\n",
       " 'gatilho',\n",
       " 'gatinha',\n",
       " 'gatorade',\n",
       " 'geladíssima',\n",
       " 'gelar',\n",
       " 'generoso',\n",
       " 'geniais',\n",
       " 'gentileza',\n",
       " 'geral',\n",
       " 'geralmente',\n",
       " 'geração',\n",
       " 'gmail',\n",
       " 'gnt',\n",
       " 'goat',\n",
       " 'gola',\n",
       " 'golpe',\n",
       " 'google',\n",
       " 'gostar',\n",
       " 'gosto',\n",
       " 'gostosa',\n",
       " 'gostosinha',\n",
       " 'gostosoooo',\n",
       " 'gracinha',\n",
       " 'grande',\n",
       " 'grandes',\n",
       " 'gratidão',\n",
       " 'graça',\n",
       " 'grupo',\n",
       " 'grátis',\n",
       " 'guarana',\n",
       " 'guarani',\n",
       " 'guaraná',\n",
       " 'guarda',\n",
       " 'guardar',\n",
       " 'gucci',\n",
       " 'guuuuhsantana',\n",
       " 'gênia',\n",
       " 'ha',\n",
       " 'hacer',\n",
       " 'hahahahaah',\n",
       " 'hahhahaa',\n",
       " 'halls',\n",
       " 'hamburguer',\n",
       " 'hardy',\n",
       " 'haste',\n",
       " 'hehe',\n",
       " 'hello',\n",
       " 'henna',\n",
       " 'hesceleb',\n",
       " 'hfcoradini',\n",
       " 'hilfiger',\n",
       " 'histórias',\n",
       " 'hombres',\n",
       " 'hora',\n",
       " 'horas',\n",
       " 'horror',\n",
       " 'horrível',\n",
       " 'house',\n",
       " 'hugmeclub',\n",
       " 'humilhação',\n",
       " 'ia',\n",
       " 'iaê',\n",
       " 'ideias',\n",
       " 'idêntico',\n",
       " 'ignoraron',\n",
       " 'igreja',\n",
       " 'igual',\n",
       " 'ijsdjskskskskakka',\n",
       " 'imaginar',\n",
       " 'imnotlyarax',\n",
       " 'importados',\n",
       " 'importar',\n",
       " 'impossível',\n",
       " 'impressionante',\n",
       " 'impressão',\n",
       " 'incluir',\n",
       " 'incrível',\n",
       " 'indecisa',\n",
       " 'inesperado',\n",
       " 'inferno',\n",
       " 'influência',\n",
       " 'informei',\n",
       " 'inicial',\n",
       " 'inimigo',\n",
       " 'inocente',\n",
       " 'insta',\n",
       " 'instabilidade',\n",
       " 'instável',\n",
       " 'intensidades',\n",
       " 'inter',\n",
       " 'interessada',\n",
       " 'interessar',\n",
       " 'internacional',\n",
       " 'intitular',\n",
       " 'iphone',\n",
       " 'ir',\n",
       " 'irar',\n",
       " 'iria',\n",
       " 'iriar',\n",
       " 'irma',\n",
       " 'irmao',\n",
       " 'irrita',\n",
       " 'irão',\n",
       " 'isabollalauck',\n",
       " 'isisofmine',\n",
       " 'isolamento',\n",
       " 'itau',\n",
       " 'iurick',\n",
       " 'jacob',\n",
       " 'jahgodomingues',\n",
       " 'janeiro',\n",
       " 'jaqueta',\n",
       " 'jasmine',\n",
       " 'jeans',\n",
       " 'jeito',\n",
       " 'jjkpjmluvx',\n",
       " 'joaoalvesjhja',\n",
       " 'joaopgatti',\n",
       " 'joarlens',\n",
       " 'jogador',\n",
       " 'jogar',\n",
       " 'jogger',\n",
       " 'jogo',\n",
       " 'jordan',\n",
       " 'julgar',\n",
       " 'juliette',\n",
       " 'julietteday',\n",
       " 'jung_dede',\n",
       " 'junho',\n",
       " 'junioraas',\n",
       " 'junto',\n",
       " 'juntão',\n",
       " 'juro',\n",
       " 'juros',\n",
       " 'jurídica',\n",
       " 'justa',\n",
       " 'juventus',\n",
       " 'kayofeer',\n",
       " 'kidman',\n",
       " 'killisabella',\n",
       " 'kit',\n",
       " 'kk',\n",
       " 'kkkkkk',\n",
       " 'kkkkkkk',\n",
       " 'kkkkkkkj',\n",
       " 'kkkkkkkkk',\n",
       " 'kkkkkkkkkkk',\n",
       " 'kkkkkkkkkkkk',\n",
       " 'kkkkkkkkkkkkk',\n",
       " 'kkkkkkkkkkkkkk',\n",
       " 'kkkkkkkkkkkkkkk',\n",
       " 'kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk',\n",
       " 'kkkkkkkkno',\n",
       " 'klein',\n",
       " 'korova',\n",
       " 'kors',\n",
       " 'ksksksskkskszksk',\n",
       " 'kwai',\n",
       " 'l_cocate',\n",
       " 'la',\n",
       " 'lacoste',\n",
       " 'lala',\n",
       " 'lan',\n",
       " 'lançar',\n",
       " 'larahino',\n",
       " 'largar',\n",
       " 'larguei',\n",
       " 'las',\n",
       " 'latinha',\n",
       " 'legadaodamassa',\n",
       " 'legal',\n",
       " 'leite',\n",
       " 'leitecristiane',\n",
       " 'lekinhobala',\n",
       " 'lembrar',\n",
       " 'lembro',\n",
       " 'lenir',\n",
       " 'lento',\n",
       " 'leomfermiano',\n",
       " 'ler',\n",
       " 'liberar',\n",
       " 'liberto',\n",
       " 'ligar',\n",
       " 'limiteee',\n",
       " 'limiteeeeeee',\n",
       " 'limites',\n",
       " 'limpa',\n",
       " 'limão',\n",
       " 'linda',\n",
       " 'lindar',\n",
       " 'lindo',\n",
       " 'lingerie',\n",
       " 'lingeries',\n",
       " 'linha',\n",
       " 'link',\n",
       " 'lipo',\n",
       " 'lisinho',\n",
       " 'lista',\n",
       " 'literalmente',\n",
       " 'litro',\n",
       " 'live',\n",
       " 'livre',\n",
       " 'lixo',\n",
       " 'lixão',\n",
       " 'lj_gfbpa',\n",
       " 'llegado',\n",
       " 'llegara',\n",
       " 'loco',\n",
       " 'logo',\n",
       " 'loja',\n",
       " 'lojas',\n",
       " 'lojinhas',\n",
       " 'lol',\n",
       " 'longo',\n",
       " 'lord',\n",
       " 'lorenachss',\n",
       " 'louco',\n",
       " 'louietbrve',\n",
       " 'lt',\n",
       " 'lucasrampanellj',\n",
       " 'lugar',\n",
       " 'lugares',\n",
       " 'luhquennehen',\n",
       " 'luizesampaio',\n",
       " 'luizzucolotto',\n",
       " 'luís',\n",
       " 'lá',\n",
       " 'líquido',\n",
       " 'maaaas',\n",
       " 'magalzaoshow',\n",
       " 'magnetis',\n",
       " 'mah',\n",
       " 'mai',\n",
       " 'mail',\n",
       " 'mainha',\n",
       " 'maio',\n",
       " 'maior',\n",
       " 'mal',\n",
       " 'malas',\n",
       " 'maldito',\n",
       " 'malhar',\n",
       " 'mallanlets',\n",
       " 'maluco',\n",
       " 'maluzenhx',\n",
       " 'mamar',\n",
       " 'mamda',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names() # Vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_.get('comprar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaaa</th>\n",
       "      <th>abandonar</th>\n",
       "      <th>abençoar</th>\n",
       "      <th>abismal</th>\n",
       "      <th>abrir</th>\n",
       "      <th>abro</th>\n",
       "      <th>absurda</th>\n",
       "      <th>absurdo</th>\n",
       "      <th>abóbora</th>\n",
       "      <th>acabar</th>\n",
       "      <th>...</th>\n",
       "      <th>ânimo</th>\n",
       "      <th>éo</th>\n",
       "      <th>época</th>\n",
       "      <th>ódio</th>\n",
       "      <th>óleo</th>\n",
       "      <th>ótima</th>\n",
       "      <th>ótimo</th>\n",
       "      <th>última</th>\n",
       "      <th>únicas</th>\n",
       "      <th>único</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaaaaa  abandonar  abençoar  abismal  abrir  abro  absurda  absurdo  \\\n",
       "0       0          0         0        0      0     0        0        0   \n",
       "1       0          0         0        0      0     0        0        0   \n",
       "2       0          0         0        0      0     0        0        0   \n",
       "3       0          0         0        0      0     0        0        0   \n",
       "4       0          0         0        0      0     0        0        0   \n",
       "\n",
       "   abóbora  acabar  ...  ânimo  éo  época  ódio  óleo  ótima  ótimo  última  \\\n",
       "0        0       0  ...      0   0      0     0     0      0      0       0   \n",
       "1        0       0  ...      0   0      0     0     0      0      0       0   \n",
       "2        0       0  ...      0   0      0     0     0      0      0       0   \n",
       "3        0       0  ...      0   0      0     0     0      0      0       0   \n",
       "4        0       0  ...      0   0      0     0     0      0      0       0   \n",
       "\n",
       "   únicas  único  \n",
       "0       0      0  \n",
       "1       0      0  \n",
       "2       0      0  \n",
       "3       0      0  \n",
       "4       0      0  \n",
       "\n",
       "[5 rows x 1783 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv = pd.DataFrame(X.toarray(), columns = count_vectorizer.get_feature_names())\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X.toarray()\n",
    "y = df_dataset['classified']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Criando o Modelo Naive Bayes \n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "#.......Treinando o Modelo.......\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "#Fazendo as previsões\n",
    "naive_bayes_pred = naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo 61.19402985074627\n",
      "\n",
      "Matriz de confusão: \n",
      " [[31 22]\n",
      " [30 51]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "#Calculando a acurácia\n",
    "acc = accuracy_score(naive_bayes_pred, y_test)\n",
    "\n",
    "#Matriz de confusão \n",
    "cm = confusion_matrix(naive_bayes_pred, y_test)\n",
    "\n",
    "print(\"Acurácia do modelo\", acc*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "nova_frase = [\"ódio da nubank quando tem notificação e é propaganda ao invés de ser dinheiro caindo na minha conta\"] \n",
    "teste = count_vectorizer.transform(nova_frase)\n",
    "pred = naive_bayes.predict(teste)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "nova_frase2 = [\"Chegou o meu cartão do @nubank. Tô amando a embalagem dele Cara sorridente com os olhos em forma de coração\"] \n",
    "teste2 = count_vectorizer.transform(nova_frase2)\n",
    "pred2 = naive_bayes.predict(teste2)\n",
    "print(pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando datasets por Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nubank = df_dataset[['Nubank' in x for x in df_dataset['keyword']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nike = df_dataset[['Nike' in x for x in df_dataset['keyword']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shein = df_dataset[['SHEIN' in x for x in df_dataset['keyword']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nubank_count_vectorizer = CountVectorizer(binary=True)\n",
    "#X_nubank = nubank_count_vectorizer.fit_transform(df_nubank['tweet_content'].values.astype('U'))\n",
    "#X_nubank = X_nubank.toarray()\n",
    "\n",
    "X_nubank = df_nubank['tweet_content']\n",
    "y_nubank = df_nubank['classified']\n",
    "\n",
    "X_train_nubank, X_test_nubank, y_train_nubank, y_test_nubank = train_test_split(X_nubank, y_nubank, test_size=0.3, \n",
    "                                                                                random_state=42, stratify=y_nubank)\n",
    "X_train_nubank = nubank_count_vectorizer.fit_transform(X_train_nubank)\n",
    "X_test_nubank = nubank_count_vectorizer.transform(X_test_nubank.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nike_count_vectorizer = CountVectorizer(binary=True)\n",
    "#X_nike = nike_count_vectorizer.fit_transform(df_nike['tweet_content'].values.astype('U'))\n",
    "\n",
    "X_nike = df_nike['tweet_content']\n",
    "y_nike = df_nike['classified']\n",
    "\n",
    "X_train_nike, X_test_nike, y_train_nike, y_test_nike = train_test_split(X_nike, y_nike, test_size=0.3, \n",
    "                                                                                random_state=42, stratify=y_nike)\n",
    "X_train_nike = nike_count_vectorizer.fit_transform(X_train_nike.values.astype('U'))\n",
    "X_test_nike = nike_count_vectorizer.transform(X_test_nike.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "shein_count_vectorizer = CountVectorizer(binary=True)\n",
    "#X_shein = shein_count_vectorizer.fit_transform(df_shein['tweet_content'].values.astype('U'))\n",
    "\n",
    "X_shein = df_shein['tweet_content']\n",
    "y_shein = df_shein['classified']\n",
    "\n",
    "X_train_shein, X_test_shein, y_train_shein, y_test_shein = train_test_split(X_shein, y_shein, test_size=0.3, \n",
    "                                                                                random_state=42, stratify=y_shein)\n",
    "X_train_shein = shein_count_vectorizer.fit_transform(X_train_shein.values.astype('U'))\n",
    "X_test_shein = shein_count_vectorizer.transform(X_test_shein.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando MultinomialNB - Nubank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 43.99999999999999\n",
      "\n",
      "Matriz de confusão: \n",
      " [[28 19]\n",
      " [ 9 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Criando o Modelo Naive Bayes \n",
    "naive_bayes_nubank = MultinomialNB()\n",
    "#.......Treinando o Modelo.......\n",
    "naive_bayes_nubank.fit(X_train_nubank, y_train_nubank)\n",
    "#Fazendo as previsões\n",
    "naive_bayes_pred_nubank = naive_bayes_nubank.predict(X_test_nubank)\n",
    "\n",
    "f_medida_nubank = f1_score(y_test_nubank, naive_bayes_pred_nubank)\n",
    "cm_nubank = confusion_matrix(naive_bayes_pred_nubank, y_test_nubank)\n",
    "\n",
    "print(\"F-medida:\", f_medida_nubank*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_nubank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial NB - Nike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 65.625\n",
      "\n",
      "Matriz de confusão: \n",
      " [[11  9]\n",
      " [13 21]]\n"
     ]
    }
   ],
   "source": [
    "#Criando o Modelo Naive Bayes \n",
    "naive_bayes_nike = MultinomialNB()\n",
    "#.......Treinando o Modelo.......\n",
    "naive_bayes_nike.fit(X_train_nike, y_train_nike)\n",
    "#Fazendo as previsões\n",
    "naive_bayes_pred_nike = naive_bayes_nike.predict(X_test_nike)\n",
    "\n",
    "f_medida_nike = f1_score(y_test_nike, naive_bayes_pred_nike)\n",
    "cm_nike = confusion_matrix(naive_bayes_pred_nike, y_test_nike)\n",
    "\n",
    "print(\"F-medida:\", f_medida_nike*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_nike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB - SHEIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 75.75757575757576\n",
      "\n",
      "Matriz de confusão: \n",
      " [[ 5  3]\n",
      " [13 25]]\n"
     ]
    }
   ],
   "source": [
    "#Criando o Modelo Naive Bayes \n",
    "naive_bayes_shein = MultinomialNB()\n",
    "#.......Treinando o Modelo.......\n",
    "naive_bayes_shein.fit(X_train_shein, y_train_shein)\n",
    "#Fazendo as previsões\n",
    "naive_bayes_pred_shein = naive_bayes_shein.predict(X_test_shein)\n",
    "\n",
    "f_medida_shein = f1_score(y_test_shein, naive_bayes_pred_shein)\n",
    "cm_shein = confusion_matrix(naive_bayes_pred_shein, y_test_shein)\n",
    "\n",
    "print(\"F-medida:\", f_medida_shein*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_shein)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC - Nubank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 38.46153846153846\n",
      "\n",
      "Matriz de confusão: \n",
      " [[25 20]\n",
      " [12 10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.SVC(C=1.0, kernel='linear')\n",
    "SVM.fit(X_train_nubank, y_train_nubank)\n",
    "predictions_SVM_nubank = SVM.predict(X_test_nubank)\n",
    "\n",
    "f_medida_nubank_svm = f1_score(y_test_nubank, predictions_SVM_nubank)\n",
    "cm_nubank_svm = confusion_matrix(predictions_SVM_nubank, y_test_nubank)\n",
    "\n",
    "print(\"F-medida:\", f_medida_nubank_svm*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_nubank_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC - Nike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 65.71428571428571\n",
      "\n",
      "Matriz de confusão: \n",
      " [[ 7  7]\n",
      " [17 23]]\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear')\n",
    "SVM.fit(X_train_nike, y_train_nike)\n",
    "predictions_SVM_nike = SVM.predict(X_test_nike)\n",
    "\n",
    "f_medida_nike_svm = f1_score(y_test_nike, predictions_SVM_nike)\n",
    "cm_nike_svm = confusion_matrix(predictions_SVM_nike, y_test_nike)\n",
    "\n",
    "print(\"F-medida:\", f_medida_nike_svm*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_nike_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC - SHEIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 74.2857142857143\n",
      "\n",
      "Matriz de confusão: \n",
      " [[ 2  2]\n",
      " [16 26]]\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear')\n",
    "SVM.fit(X_train_shein, y_train_shein)\n",
    "predictions_SVM_shein = SVM.predict(X_test_shein)\n",
    "\n",
    "f_medida_shein_svm = f1_score(y_test_shein, predictions_SVM_shein)\n",
    "cm_shein_svm = confusion_matrix(predictions_SVM_shein, y_test_shein)\n",
    "\n",
    "print(\"F-medida:\", f_medida_shein_svm*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_shein_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Nubank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 30.769230769230766\n",
      "\n",
      "Matriz de confusão: \n",
      " [[34 24]\n",
      " [ 3  6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train_nubank, y_train_nubank)\n",
    "predictions_rfc_nubank = rfc.predict(X_test_nubank)\n",
    "\n",
    "f_medida_nubank_rfc = f1_score(y_test_nubank, predictions_rfc_nubank)\n",
    "cm_nubank_rfc = confusion_matrix(predictions_rfc_nubank, y_test_nubank)\n",
    "\n",
    "print(\"F-medida:\", f_medida_nubank_rfc*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_nubank_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Nike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 68.35443037974683\n",
      "\n",
      "Matriz de confusão: \n",
      " [[ 2  3]\n",
      " [22 27]]\n"
     ]
    }
   ],
   "source": [
    "rfc.fit(X_train_nike, y_train_nike)\n",
    "predictions_rfc_nike = rfc.predict(X_test_nike)\n",
    "\n",
    "f_medida_nike_rfc = f1_score(y_test_nike, predictions_rfc_nike)\n",
    "cm_nike_rfc = confusion_matrix(predictions_rfc_nike, y_test_nike)\n",
    "\n",
    "print(\"F-medida:\", f_medida_nike_rfc*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_nike_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - SHEIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 75.67567567567568\n",
      "\n",
      "Matriz de confusão: \n",
      " [[ 0  0]\n",
      " [18 28]]\n"
     ]
    }
   ],
   "source": [
    "rfc.fit(X_train_shein, y_train_shein)\n",
    "predictions_rfc_shein = rfc.predict(X_test_shein)\n",
    "\n",
    "f_medida_shein_rfc = f1_score(y_test_shein, predictions_rfc_shein)\n",
    "cm_shein_rfc = confusion_matrix(predictions_rfc_shein, y_test_shein)\n",
    "\n",
    "print(\"F-medida:\", f_medida_shein_rfc*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_shein_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - Nubank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 45.00000000000001\n",
      "\n",
      "Matriz de confusão: \n",
      " [[36 21]\n",
      " [ 1  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(X_train_nubank, y_train_nubank)\n",
    "\n",
    "predictions_knn_nubank = knn.predict(X_test_nubank)\n",
    "\n",
    "f_medida_nubank_knn = f1_score(y_test_nubank, predictions_knn_nubank)\n",
    "cm_nubank_knn = confusion_matrix(predictions_knn_nubank, y_test_nubank)\n",
    "\n",
    "print(\"F-medida:\", f_medida_nubank_knn*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_nubank_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - Nike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 71.05263157894737\n",
      "\n",
      "Matriz de confusão: \n",
      " [[ 5  3]\n",
      " [19 27]]\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train_nike, y_train_nike)\n",
    "\n",
    "predictions_knn_nike = knn.predict(X_test_nike)\n",
    "\n",
    "f_medida_nike_knn = f1_score(y_test_nike, predictions_knn_nike)\n",
    "cm_nike_knn = confusion_matrix(predictions_knn_nike, y_test_nike)\n",
    "\n",
    "print(\"F-medida:\", f_medida_nike_knn*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_nike_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN - SHEIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 57.14285714285715\n",
      "\n",
      "Matriz de confusão: \n",
      " [[11 14]\n",
      " [ 7 14]]\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train_shein, y_train_shein)\n",
    "\n",
    "predictions_knn_shein = knn.predict(X_test_shein)\n",
    "\n",
    "f_medida_shein_knn = f1_score(y_test_shein, predictions_knn_shein)\n",
    "cm_shein_knn = confusion_matrix(predictions_knn_shein, y_test_shein)\n",
    "\n",
    "print(\"F-medida:\", f_medida_shein_knn*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_shein_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando os modelos no dataset inteiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 66.23376623376623\n",
      "\n",
      "Matriz de confusão: \n",
      " [[31 22]\n",
      " [30 51]]\n"
     ]
    }
   ],
   "source": [
    "#Criando o Modelo Naive Bayes \n",
    "naive_bayes = MultinomialNB()\n",
    "#.......Treinando o Modelo.......\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "#Fazendo as previsões\n",
    "naive_bayes_pred = naive_bayes.predict(X_test)\n",
    "\n",
    "f_medida = f1_score(y_test, naive_bayes_pred)\n",
    "cm = confusion_matrix(naive_bayes_pred, y_test)\n",
    "\n",
    "print(\"F-medida:\", f_medida*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 66.25766871165644\n",
      "\n",
      "Matriz de confusão: \n",
      " [[25 19]\n",
      " [36 54]]\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear')\n",
    "SVM.fit(X_train, y_train)\n",
    "predictions_SVM = SVM.predict(X_test)\n",
    "\n",
    "f_medida_svm = f1_score(y_test, predictions_SVM)\n",
    "cm_svm = confusion_matrix(predictions_SVM, y_test)\n",
    "\n",
    "print(\"F-medida:\", f_medida_svm*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 70.05649717514123\n",
      "\n",
      "Matriz de confusão: \n",
      " [[19 11]\n",
      " [42 62]]\n"
     ]
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)\n",
    "predictions_rfc = rfc.predict(X_test)\n",
    "\n",
    "f_medida_rfc = f1_score(y_test, predictions_rfc)\n",
    "cm_rfc = confusion_matrix(predictions_rfc, y_test)\n",
    "\n",
    "print(\"F-medida:\", f_medida_rfc*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-medida: 56.94444444444443\n",
      "\n",
      "Matriz de confusão: \n",
      " [[31 32]\n",
      " [30 41]]\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "\n",
    "predictions_knn = knn.predict(X_test)\n",
    "\n",
    "f_medida_knn = f1_score(y_test, predictions_knn)\n",
    "cm_knn = confusion_matrix(predictions_knn, y_test)\n",
    "\n",
    "print(\"F-medida:\", f_medida_knn*100)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
